Joseph McDonald - Homework 1


In int_ring, I structured the program so that the argument giveen to the
executable is the number of loops around the process ring. So if there are 10
processes and N = 1000, then 10,000 messages will be passed. Latency is
measured as the time for each Send/Receive pair, or time/N*P, where P is the
number of processes.

I tested int_ring on a single CIMS box first using 4 processes, which
consistently gave a latency on the order of 1 microsecond. I then tried 8
processes which regularly gave microsecond latencies but would occasionally
require times around 1 millisecond. Running with 10 processes consistently had
latencies on the order of 1 millisecond. For this reason I suspect my box has
8 cores. Requiring 20 processes consistently took my box around 5 milliseconds
to send and receive a message.

I then tested int_ring on crunchy1 for 20 processes and got latencies that
would regularly vary between several microseconds to a few hundred
microseconds, still noticeably faster than the performance on my personal box.
Splitting the 20 processes between crunchy1 and crunchy 3 consistently
required around 150 microseconds.

After modifying the code to pass a large array around the processors on my
box, I found that the bandwidth in (bytes per second) would range from a few
hundred to a few thousand megabytes per second. When increasing the number of
processes to 10 this would inhibit the bandwidth to between 50 to a few
hundred MB per second.



In jacobi-mpi, I use the same Jacobi iterator from Homework 0 and, after
implementing for multiple processors, checked the residual and a point in the
middle to ensure it was still converging as expected. To test performance and
observe scaling I tried running the program on both my own box and a couple of
the crunchy boxes for a maximum of 10 iterations on a grid of 10^8 points. I
did observe strong scaling, first starting with 4 processes on crunchy which
took between 5 to 10 seconds. Increasing the processes to 5, then 8 and 10,
and finally 20, improvements in speed were noticeable with 20 processes across
three boxes requiring about 1.5 seconds. 8 and 10 processes both took about
2.5 to 3 seconds, while 5 processes was pretty similar to 4.

The Jacobi iteration is pretty easy to implement in parallel since the vector
is updated only at the end of each iteration. Gauss-Seidel would be much more
difficult since the solution is updated within the loop of each iteration.



